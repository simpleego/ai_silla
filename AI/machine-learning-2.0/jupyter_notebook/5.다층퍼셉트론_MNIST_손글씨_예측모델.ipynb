{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wikibook/machine-learning/blob/2.0/jupyter_notebook/5.다층퍼셉트론_MNIST_손글씨_예측모델.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# 항상 같은 결과를 갖기 위해 랜덤 시드 설정\n",
    "tf.random.set_seed(678)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다층퍼셉트론 구조\n",
    "텐서플로우로 아래의 다층퍼셉트론을 구현해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/captainchargers/deeplearning/master/img/dropout.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/captainchargers/deeplearning/master/img/dropout.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data has 60000 samples\n",
      "every train data is 28 * 28 image\n"
     ]
    }
   ],
   "source": [
    "print(\"train data has \" + str(x_train.shape[0]) + \" samples\")\n",
    "print(\"every train data is \" + str(x_train.shape[1]) \n",
    "      + \" * \" + str(x_train.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0** 부터 **255** 까지의 그레이 스케일을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# sample to show gray scale values\n",
    "print(x_train[0][8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0** 부터 **9**까지의 이미지에 해당하는 숫자를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1]\n"
     ]
    }
   ],
   "source": [
    "# sample to show labels for first train data to 10th train data\n",
    "print(y_train[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 데이터는 **10000** 개의 샘플을 가지고 있습니다.  \n",
    "모든 테스트 데이터는 **28 * 28** 의 이미지입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data has 10000 samples\n",
      "every test data is 28 * 28 image\n"
     ]
    }
   ],
   "source": [
    "print(\"test data has \" + str(x_test.shape[0]) + \" samples\")\n",
    "print(\"every test data is \" + str(x_test.shape[1]) \n",
    "      + \" * \" + str(x_test.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 정규화\n",
    "데이터 정규화는 보통 학습 시간을 단축하고, 더 나은 성능을 구하도록 도와줍니다.  \n",
    "MNIST 데이터의 모든 값은 0부터 255의 범위 안에 있으므로, 255로 값을 나눠줌으로써, 모든 값을 0부터 1 사이의 값으로 정규화합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "gray_scale = 255\n",
    "x_train /= gray_scale\n",
    "x_test /= gray_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서플로우 다층퍼셉트론 구현하기\n",
    "텐서플로우로 아래의 다층퍼셉트론을 구현해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/captainchargers/deeplearning/master/img/dropout.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/captainchargers/deeplearning/master/img/dropout.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2차원 데이터를 1차원 데이터로 변경하기 (Flatten)\n",
    "다층퍼셉트론의 입력 레이어에 데이터를 넣기 위해서 2d tensor (28, 28)인 데이터를,  \n",
    "1d tensor (28*28, 1)의  형태로 바꿔줍니다.  \n",
    "이 말은 행렬 형태의 데이터를 배열 형태의 데이터로 변경한다는 의미와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/captainchargers/deeplearning/master/img/reshape_mnist.png\" width=\"300\" height=\"150\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/captainchargers/deeplearning/master/img/reshape_mnist.png\", width=300, height=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)), # 데이터 차원 변경\n",
    "    Dense(256, activation='relu'), # 첫번째 히든 레이어 (h1)\n",
    "    Dense(128, activation='relu'), # 두번째 히든 레이어 (h2)\n",
    "    Dropout(0.1), # 두번째 히든 레이어(h2)에 드랍아웃(10%) 적용\n",
    "    Dense(10), # 세번째 히든 레이어 (logit)\n",
    "    Activation('softmax') # softmax layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 235,146\n",
      "Trainable params: 235,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번째 레이어에 784개의 입력을 받는 256개의 노드가 존재하고, 노드마다 편향값 하나씩 존재하므로,  \n",
    "784 * 256 + 256 = 200960의 파라미터가 존재합니다.  \n",
    "flatten과 softmax는 노드가 없으므로 파라미터가 존재하지 않는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sparse_categorical_crossentropy:\n",
    "레이블을 원 핫 인코딩으로 자동으로 변경하여 크로스 엔트로피 측정합니다.\n",
    "\"\"\"\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 조기 종료 (Early Stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매 주기(Epoch)마다 검증 데이터로 검증 정확도를 측정합니다.  \n",
    "검증 정확도가 5번 연속으로 개선되지 않을 시, 조기 종료를 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=False),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/300\n",
      "54000/54000 [==============================] - 3s 50us/sample - loss: 9.1650 - accuracy: 0.7719 - val_loss: 1.0220 - val_accuracy: 0.9012\n",
      "Epoch 2/300\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 1.0137 - accuracy: 0.8652 - val_loss: 0.5348 - val_accuracy: 0.8920\n",
      "Epoch 3/300\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5493 - accuracy: 0.8835 - val_loss: 0.4011 - val_accuracy: 0.9158\n",
      "Epoch 4/300\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.4023 - accuracy: 0.9051 - val_loss: 0.3420 - val_accuracy: 0.9273\n",
      "Epoch 5/300\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.3081 - accuracy: 0.9229 - val_loss: 0.3154 - val_accuracy: 0.9333\n",
      "Epoch 6/300\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2491 - accuracy: 0.9337 - val_loss: 0.2814 - val_accuracy: 0.9430\n",
      "Epoch 7/300\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2039 - accuracy: 0.9438 - val_loss: 0.2770 - val_accuracy: 0.9453\n",
      "Epoch 8/300\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1779 - accuracy: 0.9488 - val_loss: 0.2705 - val_accuracy: 0.9480\n",
      "Epoch 9/300\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1504 - accuracy: 0.9556 - val_loss: 0.2608 - val_accuracy: 0.9510\n",
      "Epoch 10/300\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1332 - accuracy: 0.9599 - val_loss: 0.2610 - val_accuracy: 0.9520\n",
      "Epoch 11/300\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.1185 - accuracy: 0.9633 - val_loss: 0.2598 - val_accuracy: 0.9530\n",
      "Epoch 12/300\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.1047 - accuracy: 0.9669 - val_loss: 0.2560 - val_accuracy: 0.9575\n",
      "Epoch 13/300\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.0960 - accuracy: 0.9690 - val_loss: 0.2566 - val_accuracy: 0.9557\n",
      "Epoch 14/300\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.0849 - accuracy: 0.9720 - val_loss: 0.2493 - val_accuracy: 0.9560\n",
      "Epoch 15/300\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.0804 - accuracy: 0.9742 - val_loss: 0.2623 - val_accuracy: 0.9585\n",
      "Epoch 16/300\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.0713 - accuracy: 0.9758 - val_loss: 0.2502 - val_accuracy: 0.9548\n",
      "Epoch 17/300\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.0661 - accuracy: 0.9786 - val_loss: 0.2621 - val_accuracy: 0.9567\n",
      "Epoch 18/300\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.0616 - accuracy: 0.9794 - val_loss: 0.2527 - val_accuracy: 0.9600\n",
      "Epoch 19/300\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.0551 - accuracy: 0.9812 - val_loss: 0.2501 - val_accuracy: 0.9595\n",
      "Epoch 20/300\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.0564 - accuracy: 0.9806 - val_loss: 0.2516 - val_accuracy: 0.9593\n",
      "Epoch 21/300\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.0498 - accuracy: 0.9832 - val_loss: 0.2532 - val_accuracy: 0.9612\n",
      "Epoch 22/300\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.0459 - accuracy: 0.9847 - val_loss: 0.2622 - val_accuracy: 0.9605\n",
      "Epoch 23/300\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.0454 - accuracy: 0.9845 - val_loss: 0.2719 - val_accuracy: 0.9620\n",
      "Epoch 24/300\n",
      "54000/54000 [==============================] - 2s 44us/sample - loss: 0.0458 - accuracy: 0.9844 - val_loss: 0.2485 - val_accuracy: 0.9632\n",
      "Epoch 25/300\n",
      "54000/54000 [==============================] - 2s 44us/sample - loss: 0.0448 - accuracy: 0.9852 - val_loss: 0.2538 - val_accuracy: 0.9620\n",
      "Epoch 26/300\n",
      "54000/54000 [==============================] - 3s 46us/sample - loss: 0.0423 - accuracy: 0.9854 - val_loss: 0.2432 - val_accuracy: 0.9650\n",
      "Epoch 27/300\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.0374 - accuracy: 0.9872 - val_loss: 0.2565 - val_accuracy: 0.9647\n",
      "Epoch 28/300\n",
      "54000/54000 [==============================] - 2s 43us/sample - loss: 0.0359 - accuracy: 0.9874 - val_loss: 0.2626 - val_accuracy: 0.9645\n",
      "Epoch 29/300\n",
      "54000/54000 [==============================] - 3s 47us/sample - loss: 0.0361 - accuracy: 0.9878 - val_loss: 0.2602 - val_accuracy: 0.9648\n",
      "Epoch 30/300\n",
      "54000/54000 [==============================] - 3s 49us/sample - loss: 0.0326 - accuracy: 0.9888 - val_loss: 0.2599 - val_accuracy: 0.9650\n",
      "Epoch 31/300\n",
      "54000/54000 [==============================] - 3s 49us/sample - loss: 0.0342 - accuracy: 0.9887 - val_loss: 0.2684 - val_accuracy: 0.9647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x643554090>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=300, batch_size=1000, validation_split = 0.1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트\n",
    "검증 정확도가 가장 높은 모델을 대상으로 테스트를 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test,  y_test, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss, test acc: [0.283340558999399, 0.9599]\n"
     ]
    }
   ],
   "source": [
    "print('test loss, test acc:', results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
